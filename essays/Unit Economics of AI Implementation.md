# Unit Economics of AI Implementation

## Introduction: The Missing Layer

Most AI advice sounds like this: "AI can write your emails, summarize your documents, generate your code, automate your workflows." All true. None of it useful without the next question: **at what cost, and for what return?**

The gap between "AI can do this" and "AI should do this" is economics. You wouldn't hire an employee without knowing their salary and expected output. You wouldn't buy equipment without calculating payback period. Yet most people adopt AI tools based on demos and vibes, then wonder why their API bill surprises them or why the "time savings" never materialized.

This essay provides the framework for making AI implementation decisions on economic grounds. Not because money is the only thing that matters, but because you cannot optimize what you don't measure. And if you're running a business, every dollar and hour spent on AI is a dollar and hour not spent elsewhere.

I run a web development business—$800 website builds, $120/month maintenance for local service businesses. Every workflow I add has to pay for itself. The frameworks in this essay aren't theoretical; they're what I use to decide whether an AI implementation makes my business money or costs me money disguised as "efficiency."

The framework has three parts: understanding costs (what you put in), understanding value (what you get out), and comparing them intelligently (when to proceed, when to pause, when to optimize). We'll end with cost optimization tactics and—importantly—when to ignore cost entirely.

A caveat before we start: this framework has costs too. Don't spend an hour calculating whether a $0.50 API call is worth it. The goal is systematic thinking for decisions that matter, not bureaucracy for everything.

---

## Part 1: The Cost Model

AI implementation costs fall into five categories. Most people only count the first one.

### Token Costs

The most visible cost. You pay per token processed—both input (what you send) and output (what you receive). A token is roughly 3/4 of a word in English.

Key variables:

- **Model tier**: Frontier models (Claude 4.5 Opus, GPT-5) cost roughly 10x more per token than standard models (Claude 4.5 Sonnet), which cost roughly 10x more than fast/cheap models (Claude 4.5 Haiku)
- **Input vs output pricing**: Output tokens typically cost 3-5x more than input tokens
- **Context length**: Longer conversations cost more, and costs compound as context accumulates

**Pricing Ratios (more stable than absolute prices):**

| Model Tier | Relative Cost | Best For |
|---|---|---|
| Frontier (Opus, GPT-5) | 10x standard | Complex reasoning, high-stakes decisions |
| Standard (Sonnet, GPT-5-turbo) | 1x (baseline) | General professional work |
| Fast/Cheap (Haiku, GPT-5-mini) | 0.1x standard | High volume, simpler tasks |

Check current pricing at anthropic.com/pricing and openai.com/pricing before making decisions. The ratios between tiers tend to be more stable than absolute prices, which have dropped significantly since 2024.

### Human Time Costs

The cost that ruins most ROI calculations. You saved 30 minutes of writing, but spent 45 minutes prompting, reviewing, and fixing. Net loss.

Human time costs include:

- **Prompt development**: Getting the prompt right, especially for new use cases
- **Review time**: Reading and verifying output (this scales with output length)
- **Iteration cycles**: Back-and-forth to get acceptable results
- **Fix time**: Correcting errors the AI introduced
- **Integration time**: Getting output into the format/system you actually need

Calculate this honestly. Track your time for a week if you're not sure. Most people underestimate review time by 50% or more.

In my website business, I learned this the hard way. I built an AI workflow to generate initial client proposals. The AI drafting took 30 seconds. But I spent 15 minutes reviewing each one because I didn't trust it to get pricing right, and another 10 minutes adjusting the scope descriptions. The "AI-assisted" proposal took longer than just writing it myself with a template.

### Infrastructure Costs

For API implementations:

- Developer time to build and maintain integrations
- Hosting costs for any middleware
- Logging and monitoring systems
- Error handling and retry logic

For SaaS AI tools (ChatGPT Plus, Claude Pro, etc.):

- Subscription costs (typically $20-200/month, check current pricing)
- Often a better deal than API for individual users, worse for teams at scale

### Opportunity Cost

The silent killer. Every hour spent building an AI workflow is an hour not spent on something else. Every dollar spent on API calls is a dollar not spent on ads, contractors, or equipment.

This doesn't mean AI is wrong—it means you need to compare against alternatives:

- Could a $15/hour VA do this task?
- Could you just... not do this task?
- Could you do a simpler version without AI?

For my $120/month maintenance clients, I considered building an AI system to auto-generate monthly reports. Then I realized: a simple template with 5 screenshots takes 10 minutes and clients love it. The AI version would have taken 20 hours to build for maybe 5 minutes saved per report. Bad economics.

### Hidden Costs

The ones that don't show up until you're committed:

- **Context switching**: Jumping between AI and your actual work
- **Learning curves**: Time to understand the tool's quirks and limitations
- **Debugging**: When the AI workflow breaks at 2am
- **Dependency risk**: What happens when the API is down or pricing changes?

### Cost Calculation Template

Here's how to estimate total cost for an AI implementation:

```
COST CALCULATION: [Workflow Name]

TOKEN COSTS (per use)
- Average input tokens: _____
- Average output tokens: _____
- Model tier: _____ (check current per-token pricing)
- Expected uses per month: _____
- Monthly token cost: $_____

HUMAN TIME (per use)
- Prompt preparation: _____ minutes
- Waiting/context switch: _____ minutes
- Review time: _____ minutes
- Fix/iteration time: _____ minutes
- Total time: _____ minutes x $_____ hourly rate = $_____

INFRASTRUCTURE (monthly)
- Subscription/platform: $_____
- Hosting/maintenance: $_____
- Developer time allocation: _____ hours x $_____ = $_____

SETUP COSTS (one-time, amortized)
- Initial development: _____ hours x $_____ = $_____
- Amortized over: _____ months
- Monthly setup cost: $_____

TOTAL MONTHLY COST: $_____
COST PER USE: $_____
```

**Worked Example: AI-Assisted Client Onboarding Emails**

In my web development business, I wanted to use Claude 4.5 Sonnet to draft personalized onboarding emails for new maintenance clients.

```
TOKEN COSTS (per email)
- Input: ~1,500 tokens (client info + template prompt)
- Output: ~800 tokens (personalized email draft)
- Model: Sonnet (standard tier)
- Per-email token cost: ~$0.02-0.05 (check current pricing)
- 4 new clients/month: ~$0.20/month

HUMAN TIME (per email)
- Prep (gather client context): 5 minutes
- Review: 5 minutes
- Minor edits: 5 minutes
- Total: 15 minutes x $100/hr = $25

INFRASTRUCTURE
- Claude Pro subscription: $20/month (using chat interface)

SETUP (one-time)
- Developing email template/prompt: 2 hours x $100 = $200
- Amortized over 12 months: $17/month

TOTAL MONTHLY: $0.20 + ($25 x 4) + $20 + $17 = $137
COST PER EMAIL: ~$34
```

The token cost is trivial. The human time dominates. This is true for most knowledge work applications. See [[Intelligence as a Commodity]] for why workflow design matters more than model choice.

---

## Part 2: The Value Model

Costs are concrete. Value is slippery. Here's how to pin it down.

### Time Saved x Hourly Rate

The simplest calculation, and often misleading.

If AI saves you 2 hours on a task, and your hourly rate is $100, the value is $200. Simple.

Except: Did you actually do something productive with those 2 hours? Or did they evaporate into email and context switching? Be honest. Only count time savings if you have a realistic plan for how to use the freed time.

More accurate formula:

```
Time Value = Hours Saved x Hourly Rate x Utilization Factor

Where Utilization Factor is:
- 1.0 = You will definitely bill this time to a client
- 0.7 = You will probably use this productively
- 0.3 = This will likely become slack time
- 0.0 = You're not actually time-constrained
```

For my business, I'm usually at 0.7-1.0 because I have a backlog of client work. If you're not time-constrained, your utilization factor is closer to 0.3 and time savings are worth much less.

### Revenue Enabled

What becomes possible that wasn't before? This is often where the real value lives.

- AI lets you offer a service you couldn't before (new revenue stream)
- AI lets you take on more clients (capacity increase)
- AI lets you respond faster, winning deals you'd otherwise lose (conversion improvement)
- AI lets you personalize at scale (higher prices or better retention)

This is harder to measure but often 10x more important than time savings.

Example from my business: Claude Code lets me build custom features for clients that would have been too time-consuming before. A client wanted a custom booking integration—previously a $2,000+ project I'd decline. With AI assistance, I could do it in 4 hours for $600. That's not "time savings"—it's revenue I couldn't have captured otherwise.

### Risk Reduced

When AI catches errors or improves consistency:

- What's the cost of an error that AI prevents?
- How often would that error occur?
- Expected value = Error cost x Error probability x Reduction rate

Example: I use AI to review client proposals before sending. It catches embarrassing typos, pricing inconsistencies, and scope gaps maybe 1 in 10 proposals. Each caught error probably saves me $500 in reputation damage or rework. That's $50 expected value per proposal reviewed.

### Quality Improvement

When quality has direct economic value:

- Higher quality -> Higher prices (can you charge more?)
- Higher quality -> Better retention (do customers stay longer?)
- Higher quality -> More referrals (does quality drive word-of-mouth?)

Hard to measure precisely, but don't ignore it. Even a rough estimate beats zero.

### Optionality Value

Sometimes the value isn't immediate—it's capability you now have.

Building an AI workflow teaches you patterns you'll reuse. Having the infrastructure means you can scale quickly if demand appears. Being comfortable with AI tools means you're ready when the next opportunity emerges.

This is real value, but be careful not to use "optionality" to justify things that will never pay off. Ask: "Would I pay $X just to have this capability available?" If not, don't count it.

### Value Attribution Framework

```
VALUE CALCULATION: [Workflow Name]

TIME SAVINGS
- Hours saved per use: _____
- Hourly rate: $_____
- Utilization factor: _____
- Uses per month: _____
- Monthly time value: $_____

REVENUE ENABLED
- New revenue stream: $_____ /month
- Additional capacity: _____ units x $_____ = $_____
- Conversion improvement: _____ deals x $_____ x ____% lift = $_____
- Monthly revenue value: $_____

RISK REDUCTION
- Error cost: $_____
- Error frequency: _____ /month
- AI catch rate: _____%
- Monthly risk value: $_____

QUALITY IMPROVEMENT
- Price premium enabled: _____%
- Revenue affected: $_____
- Monthly quality value: $_____

TOTAL MONTHLY VALUE: $_____
VALUE PER USE: $_____
```

---

## Part 3: Break-Even Analysis

Now we compare.

### Simple Break-Even

If monthly value > monthly cost, proceed. If not, don't.

But this ignores timing and scale. A workflow that costs $500/month and delivers $600/month looks profitable, but:

- How long until you've recovered setup costs?
- Does it get better or worse at scale?
- What's your confidence in the value estimate?

### Time to Break-Even

```
Break-Even Time = Setup Costs / (Monthly Value - Monthly Operating Cost)
```

For my client onboarding email example, if the value is estimated at $50/email (faster onboarding, better first impression, fewer clarifying emails):

```
Monthly value: 4 emails x $50 = $200
Monthly cost: $137
Monthly profit: $63
Setup costs: $200
Break-even: 200 / 63 = ~3 months
```

Reasonable for a workflow I'll use indefinitely. But if monthly profit were $10, break-even would be 20 months—risky for a workflow that might need revision.

### Marginal vs Fixed Costs

First implementation is expensive. You're learning, building prompts, setting up infrastructure. The 100th use of the same workflow is cheap—you've already paid the fixed costs.

This means:

- Low-volume workflows need high value per use to justify setup
- High-volume workflows can tolerate lower value per use
- Reusable components (prompt libraries, integrations) are investments

**Break-Even by Use Case Type**

| Use Case Type | Typical Setup | Monthly Fixed | Per-Use Cost | Break-Even Volume |
|---|---|---|---|---|
| One-off complex task | 1-2 hours | $0 | varies by model | 1 (if value > cost) |
| Recurring simple task | 2-4 hours | $20-50 | minimal (Haiku-tier) | 50-100 uses |
| Business workflow | 10-40 hours | $50-200 | varies by complexity | 3-6 months |
| Customer-facing feature | 40-100 hours | $200-500 | minimal per query | 6-12 months |

---

## Part 4: Batch vs Real-Time Economics

Same task, radically different economics depending on timing requirements.

### When Batch Processing Wins

Batch means: collect requests, process together, deliver results later.

Batch wins when:

- **Latency tolerance**: Results needed in hours/days, not seconds
- **Volume**: Many similar requests to process
- **Cost sensitivity**: Token costs matter at your scale
- **Off-peak pricing**: Some providers offer discounts for async processing

Example: Processing customer feedback. You receive 500 feedback items per week. Real-time processing with a standard model might cost 2-3x what batch processing with a cheaper model costs, plus you need constant infrastructure monitoring. Batch processing nightly is simpler and cheaper.

```
Real-time with Sonnet: Higher cost + operational overhead
Batch with Haiku: ~60-70% cost reduction + simplified operations
```

The exact savings depend on current pricing, but the pattern holds: batch processing with cheaper models typically saves 50-70% versus real-time with premium models.

### When Real-Time Wins

Real-time means: process immediately, respond instantly.

Real-time wins when:

- **User-facing**: Someone is waiting for a response
- **Time-sensitive**: Value decays rapidly (customer support, trading signals)
- **Interactive**: Need back-and-forth refinement
- **High value per query**: Each query matters enough to pay premium

Example: AI-assisted customer support. Response time correlates with customer satisfaction and conversion. A 2-hour delay drops conversion by 15%. The math changes completely.

### Decision Tree: Batch vs Real-Time

```
START: Does someone need this in <5 minutes?
|-- YES -> Real-time
|   |-- Is per-query value high?
|       |-- YES -> Use best available model
|       |-- NO -> Use fast/cheap model, optimize prompts
|-- NO -> Can we batch similar requests?
    |-- YES -> Batch processing
    |   |-- Is total volume > 1000/day?
    |       |-- YES -> Consider dedicated infrastructure
    |       |-- NO -> Simple scheduled jobs
    |-- NO -> Process individually as they arrive
        |-- Standard real-time with queue
```

---

## Part 5: Cost Optimization Tactics

Once you've validated that a workflow makes economic sense, here's how to make it more efficient.

### Model Routing

Not every query needs your most expensive model. Build logic to route:

- Simple queries -> Haiku-tier (fast/cheap)
- Complex queries -> Opus-tier (frontier)
- Ambiguous -> Start cheap, escalate if confidence is low

A well-designed router can cut costs 40-60% with minimal quality impact. This is the "faucet" concept from [[Intelligence as a Commodity]]—matching the right capability level to each task.

### Prompt Optimization

Every token costs money. Optimize prompts by:

- Removing unnecessary instructions (test what's actually needed)
- Using shorter examples
- Compressing context (summaries instead of full documents)
- Structuring output to be concise

A 30% prompt reduction = 30% input cost reduction. Compound this across thousands of calls.

### Caching Patterns

Don't re-run identical queries:

- Hash inputs, cache outputs
- Set appropriate TTL (time-to-live) based on how often underlying data changes
- Cache at the right granularity (full response vs components)

Even simple caching often eliminates 20-40% of API calls.

### Batch Windowing

If real-time isn't required, aggregate requests:

- Collect for 5 minutes, process together
- Reduces per-request overhead
- Enables bulk pricing on some platforms

### When to Optimize vs When to Pay More

Optimization has costs: developer time, increased complexity, potential bugs.

**Optimize when:**

- Token costs are >20% of total workflow cost
- You're making >10,000 calls/month
- You've validated the workflow works and will continue

**Just pay more when:**

- You're still iterating on the workflow
- Volume is low (<1,000 calls/month)
- Optimization time exceeds 3 months of savings
- You're time-constrained, not cost-constrained

### Optimization Checklist

```
COST OPTIMIZATION REVIEW: [Workflow Name]

QUICK WINS (Do first)
[ ] Are we using the cheapest model that works?
[ ] Is caching enabled for repeated queries?
[ ] Can we reduce prompt length by 20%+ without quality loss?

MEDIUM EFFORT
[ ] Can we batch requests that don't need real-time?
[ ] Should we implement model routing?
[ ] Are we logging costs per workflow for monitoring?

ADVANCED (High volume only)
[ ] Custom fine-tuning to reduce prompt length?
[ ] Self-hosted models for high-volume simple tasks?
[ ] Dedicated capacity agreements with providers?

SANITY CHECK
[ ] Is optimization time < 3 months of expected savings?
[ ] Will this workflow still exist in 6 months?
[ ] Is cost actually our constraint, or is it quality/capability?
```

---

## Part 6: When NOT to Optimize for Cost

Cost optimization is a tool, not a religion. Here's when to ignore it.

### Capability-Limited Situations

If only the expensive model can do the task, the cheap model isn't an option. A 60% cost savings means nothing if output quality drops below usable.

Run quality comparisons before deciding. Sometimes Haiku really can do what Opus does. Sometimes it absolutely cannot. I've found that for my client communication tasks, Sonnet is usually sufficient. But for complex code generation in Claude Code, I want Opus—the time I'd spend fixing Sonnet's mistakes exceeds the cost difference.

### High-Stakes Decisions

When the cost of a wrong answer vastly exceeds token costs:

- Legal document review
- Medical information
- Financial decisions
- Security analysis

The frontier model costs a few cents more per query. A missed error costs $10,000 in rework or liability. This is not a close call.

### Learning and Exploration Phases

When you're figuring out what's possible:

- Use the best model to establish ceiling of what works
- Optimize later once you know the target
- Premature optimization wastes time if the approach changes

### Time-Sensitive Situations

When speed matters more than cost:

- Customer is waiting
- Market window is closing
- You're blocked on this output

Your time has value. Spending an hour to save $5 in API costs is bad economics. See [[Maximizing AI Utility as a Business Owner]] for more on this failure mode.

---

## Part 7: Putting It Together

### Decision Framework Flowchart

```
NEW AI IMPLEMENTATION DECISION

1. DEFINE THE TASK
   What exactly will AI do? What's the input? What's the output?
   |-- Can't define clearly? Stop. Clarify first.

2. ESTIMATE COSTS
   Use the cost calculation template.
   Total monthly cost: $_____
   |-- Is this affordable to test?
       |-- NO -> Reduce scope or find cheaper approach
       |-- YES -> Continue

3. ESTIMATE VALUE
   Use the value attribution framework.
   Total monthly value: $_____
   |-- Is value > cost by at least 2x?
       |-- NO -> Reconsider. Is value estimate too conservative?
       |         Is there a cheaper way?
       |-- YES -> Continue

4. CALCULATE BREAK-EVEN
   Time to recover setup costs: _____ months
   |-- Is break-even < 6 months?
       |-- NO -> High risk. Reduce setup cost or increase confidence.
       |-- YES -> Continue

5. ASSESS CONFIDENCE
   How certain are you about value estimate?
   |-- Low confidence -> Start with minimal viable version
   |-- High confidence -> Proceed with full implementation

6. BUILD AND MEASURE
   Implement. Track actual costs and value.
   |-- After 1 month: Actual vs estimated within 30%?
       |-- NO -> Diagnose and adjust
       |-- YES -> Continue and consider optimization

7. OPTIMIZE (if warranted)
   Is cost >20% of value and volume >1000/month?
   |-- YES -> Run optimization checklist
   |-- NO -> Don't bother
```

### Worked Example: Evaluating an AI Client Proposal Workflow

**Scenario**: I want to use AI to draft initial proposals for $800 website projects. I get about 8 serious inquiries per month.

**Step 1: Define the Task**

- Input: Client inquiry email + my services/pricing info
- Output: Draft proposal with scope, timeline, and pricing
- Clear enough. Proceed.

**Step 2: Estimate Costs**

```
Token costs: ~$0.05/proposal x 8/month = $0.40
Human time: 10 min prep + 15 min review/edit = 25 min x $100/hr x 8 = $333
Infrastructure: Claude Pro $20/month
Setup: 3 hours prompt dev x $100 = $300, amortized over 6 months = $50/month

Total monthly: $403
Per proposal: ~$50
```

**Step 3: Estimate Value**

```
Time saved: 20 min/proposal (vs writing from scratch) x $100/hr x 8 = $267
  BUT utilization factor is 0.7 (I probably use this time productively) = $187
Revenue enabled: Faster responses might win 1 extra deal per quarter
  1 deal x $800 / 3 months = $267/month
Quality: More consistent proposals, fewer errors -> call it $50/month

Total monthly value: $504
Per proposal: ~$63
```

**Step 4: Break-Even**

```
Monthly profit: $504 - $403 = $101
Setup cost: $300
Break-even: 300 / 101 = ~3 months
```

**Step 5: Confidence**

- Time savings: High confidence (easily measurable)
- Revenue lift: Medium confidence (assumption-based)
- Conservative estimate removing revenue lift: $237 value vs $403 cost = negative

**Decision**: This is marginal. The economics only work if the "faster response wins deals" assumption is correct. I should test with a minimal version first—maybe just use AI for the scope description section, not the whole proposal. See [[Data Monitoring and Warehousing for AI]] for how to set up measurement.

### Common Mistakes in AI Economics

1. **Counting only token costs**: Human time usually dominates. A "free" open-source model that takes 3x as long to get right isn't free.

2. **Ignoring review time**: The draft isn't the deliverable. The reviewed, edited, approved output is. Count all the time.

3. **Assuming time saved = value captured**: Saved time only has value if you actually use it productively. Be honest about your utilization factor.

4. **Optimizing too early**: Get it working first. You'll probably change the approach. I've built several "optimized" workflows that I abandoned within a month.

5. **Not measuring**: You can't know if your estimates were right without tracking actuals. See [[Data Monitoring and Warehousing for AI]] for practical measurement approaches.

6. **Sunk cost commitment**: If the workflow isn't working after a fair trial, kill it. The setup cost is gone either way.

---

## Resources

**Pricing References (check current rates)**

- Anthropic API: anthropic.com/pricing
- OpenAI API: openai.com/pricing
- Google AI: ai.google.dev/pricing

**Token Estimation**

- Most providers offer tokenizer tools
- Rule of thumb: 1 token = 4 characters in English, or 3/4 of a word

**Related Essays**

- [[Maximizing AI Utility as a Business Owner]] — The failure modes that make AI economics go negative
- [[Intelligence as a Commodity]] — Why workflow matters more than model choice (the faucet concept)
- [[Data Monitoring and Warehousing for AI]] — The infrastructure layer that enables measurement
- [[AI for Local Service Businesses]] — Applied economics for the $800 website tier

---

## Reflection Questions

Before your next AI implementation:

1. What is the actual per-use cost, including your time?
2. What specific value will this create, and how will you measure it?
3. What's your break-even timeline, and is it acceptable?
4. Are you optimizing for cost when you should be optimizing for capability (or vice versa)?
5. How will you know if this was a good decision in 3 months?

The goal isn't to turn every AI decision into a spreadsheet exercise. It's to build the habit of economic thinking so you scale what works, kill what doesn't, and stop fooling yourself about both.
